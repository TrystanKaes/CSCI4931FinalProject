{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Data Manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#Plotting and visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns #Some shiny visualizations\n",
    "\n",
    "#Utilities\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_score, f1_score, recall_score, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "#Artificial Neural Networks\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.layers import Dense, Activation, Dropout, BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------- Project Administration ------------\n",
    "DatasetFilepath = './GeneticSimulation/CORE_SLE_RA_Control_blood_panels.csv'\n",
    "DatasetFilepath = './Dataset/SLE_RA_Control_blood_panelsTestData.csv'\n",
    "#--------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(DatasetFilepath)\n",
    "data = data.drop(data.columns[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if True:\n",
    "    # Create correlation matrix\n",
    "    corr_matrix = data.corr().abs()\n",
    "\n",
    "    # Select upper triangle of correlation matrix\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "\n",
    "    # Find features with correlation greater than 0.95\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
    "\n",
    "    # Drop features \n",
    "    data.drop(to_drop, axis=1, inplace=True)\n",
    "#     data.to_csv('CORE_SLE_RA_Control_blood_panels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = LabelEncoder()\n",
    "int_data = label.fit_transform(data['Condition'])\n",
    "int_data = int_data.reshape(len(int_data), 1)\n",
    "\n",
    "onehot_data = OneHotEncoder(sparse=False)\n",
    "onehot_data = onehot_data.fit_transform(int_data)\n",
    "\n",
    "y = onehot_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['Name'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-116-fd256b28a89f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Condition'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   3988\u001b[0m                 \u001b[0mweight\u001b[0m  \u001b[0;36m1.0\u001b[0m     \u001b[0;36m0.8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3989\u001b[0m         \"\"\"\n\u001b[0;32m-> 3990\u001b[0;31m         return super().drop(\n\u001b[0m\u001b[1;32m   3991\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3992\u001b[0m             \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   3934\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3935\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3936\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3937\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3938\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[0;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[1;32m   3968\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3969\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3970\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3971\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3972\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   5016\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5017\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5018\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{labels[mask]} not found in axis\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5019\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5020\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['Name'] not found in axis\""
     ]
    }
   ],
   "source": [
    "x = data.drop(['Name', 'Condition'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>RAB30</th>\n",
       "      <th>NFIC</th>\n",
       "      <th>HNRNPA1</th>\n",
       "      <th>HECA</th>\n",
       "      <th>ADRA2A</th>\n",
       "      <th>DHX57</th>\n",
       "      <th>YIPF4</th>\n",
       "      <th>LAGE3</th>\n",
       "      <th>SLC11A2</th>\n",
       "      <th>...</th>\n",
       "      <th>ATP5I</th>\n",
       "      <th>TCHP</th>\n",
       "      <th>HDAC4</th>\n",
       "      <th>AP2M1</th>\n",
       "      <th>CYP11B1</th>\n",
       "      <th>CDYL2</th>\n",
       "      <th>INMT</th>\n",
       "      <th>WDR93</th>\n",
       "      <th>DNM3</th>\n",
       "      <th>RBBP4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3.176484</td>\n",
       "      <td>6.217651</td>\n",
       "      <td>12.932333</td>\n",
       "      <td>11.649055</td>\n",
       "      <td>3.307900</td>\n",
       "      <td>6.111577</td>\n",
       "      <td>6.846514</td>\n",
       "      <td>3.172566</td>\n",
       "      <td>3.547691</td>\n",
       "      <td>...</td>\n",
       "      <td>10.696304</td>\n",
       "      <td>7.192669</td>\n",
       "      <td>5.052599</td>\n",
       "      <td>10.142164</td>\n",
       "      <td>3.150822</td>\n",
       "      <td>3.205546</td>\n",
       "      <td>2.909560</td>\n",
       "      <td>2.281185</td>\n",
       "      <td>2.224344</td>\n",
       "      <td>6.011788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3.337685</td>\n",
       "      <td>7.084601</td>\n",
       "      <td>12.957544</td>\n",
       "      <td>11.407211</td>\n",
       "      <td>2.277668</td>\n",
       "      <td>6.094954</td>\n",
       "      <td>6.119469</td>\n",
       "      <td>4.326554</td>\n",
       "      <td>4.386112</td>\n",
       "      <td>...</td>\n",
       "      <td>10.729904</td>\n",
       "      <td>7.130932</td>\n",
       "      <td>4.709394</td>\n",
       "      <td>10.422971</td>\n",
       "      <td>2.664382</td>\n",
       "      <td>2.238226</td>\n",
       "      <td>2.653035</td>\n",
       "      <td>2.399193</td>\n",
       "      <td>2.234680</td>\n",
       "      <td>7.785639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2.709277</td>\n",
       "      <td>6.672811</td>\n",
       "      <td>12.928799</td>\n",
       "      <td>11.676965</td>\n",
       "      <td>2.372775</td>\n",
       "      <td>6.032964</td>\n",
       "      <td>7.695579</td>\n",
       "      <td>3.658660</td>\n",
       "      <td>4.261204</td>\n",
       "      <td>...</td>\n",
       "      <td>10.289376</td>\n",
       "      <td>6.888741</td>\n",
       "      <td>4.257237</td>\n",
       "      <td>9.830656</td>\n",
       "      <td>2.664382</td>\n",
       "      <td>2.238226</td>\n",
       "      <td>2.639401</td>\n",
       "      <td>2.235161</td>\n",
       "      <td>2.954536</td>\n",
       "      <td>8.514379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3.176484</td>\n",
       "      <td>6.148654</td>\n",
       "      <td>13.083771</td>\n",
       "      <td>11.841469</td>\n",
       "      <td>2.536921</td>\n",
       "      <td>6.111577</td>\n",
       "      <td>8.008814</td>\n",
       "      <td>3.105723</td>\n",
       "      <td>3.306101</td>\n",
       "      <td>...</td>\n",
       "      <td>10.899441</td>\n",
       "      <td>8.645110</td>\n",
       "      <td>4.538814</td>\n",
       "      <td>9.461622</td>\n",
       "      <td>2.661650</td>\n",
       "      <td>2.275715</td>\n",
       "      <td>2.887727</td>\n",
       "      <td>2.281185</td>\n",
       "      <td>2.224344</td>\n",
       "      <td>6.682733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>3.038200</td>\n",
       "      <td>7.006673</td>\n",
       "      <td>12.880925</td>\n",
       "      <td>11.335277</td>\n",
       "      <td>2.295058</td>\n",
       "      <td>6.051544</td>\n",
       "      <td>6.335378</td>\n",
       "      <td>4.129643</td>\n",
       "      <td>4.083541</td>\n",
       "      <td>...</td>\n",
       "      <td>10.601442</td>\n",
       "      <td>7.433140</td>\n",
       "      <td>4.051288</td>\n",
       "      <td>9.726606</td>\n",
       "      <td>2.827227</td>\n",
       "      <td>2.238226</td>\n",
       "      <td>2.809803</td>\n",
       "      <td>2.235161</td>\n",
       "      <td>2.234680</td>\n",
       "      <td>8.031372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>75</td>\n",
       "      <td>2.376699</td>\n",
       "      <td>5.598443</td>\n",
       "      <td>12.734216</td>\n",
       "      <td>10.948214</td>\n",
       "      <td>2.357735</td>\n",
       "      <td>5.895650</td>\n",
       "      <td>5.775188</td>\n",
       "      <td>2.671846</td>\n",
       "      <td>3.357576</td>\n",
       "      <td>...</td>\n",
       "      <td>9.409788</td>\n",
       "      <td>6.993906</td>\n",
       "      <td>3.580868</td>\n",
       "      <td>7.977729</td>\n",
       "      <td>2.462509</td>\n",
       "      <td>2.298061</td>\n",
       "      <td>2.937310</td>\n",
       "      <td>2.300193</td>\n",
       "      <td>2.298061</td>\n",
       "      <td>8.283852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>76</td>\n",
       "      <td>2.376699</td>\n",
       "      <td>5.971997</td>\n",
       "      <td>12.778174</td>\n",
       "      <td>10.910101</td>\n",
       "      <td>2.544198</td>\n",
       "      <td>6.092561</td>\n",
       "      <td>6.061663</td>\n",
       "      <td>2.700400</td>\n",
       "      <td>3.211287</td>\n",
       "      <td>...</td>\n",
       "      <td>9.098858</td>\n",
       "      <td>6.457562</td>\n",
       "      <td>3.196260</td>\n",
       "      <td>8.442538</td>\n",
       "      <td>2.811841</td>\n",
       "      <td>2.329875</td>\n",
       "      <td>2.937310</td>\n",
       "      <td>2.300193</td>\n",
       "      <td>2.298061</td>\n",
       "      <td>8.444267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>77</td>\n",
       "      <td>2.376699</td>\n",
       "      <td>6.298010</td>\n",
       "      <td>12.562975</td>\n",
       "      <td>10.827222</td>\n",
       "      <td>2.376818</td>\n",
       "      <td>5.776021</td>\n",
       "      <td>5.775158</td>\n",
       "      <td>2.659445</td>\n",
       "      <td>3.040549</td>\n",
       "      <td>...</td>\n",
       "      <td>9.332262</td>\n",
       "      <td>6.662107</td>\n",
       "      <td>3.333846</td>\n",
       "      <td>8.287376</td>\n",
       "      <td>3.020280</td>\n",
       "      <td>2.329875</td>\n",
       "      <td>2.306937</td>\n",
       "      <td>2.300193</td>\n",
       "      <td>2.298061</td>\n",
       "      <td>7.939047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>78</td>\n",
       "      <td>2.376699</td>\n",
       "      <td>6.177901</td>\n",
       "      <td>12.437447</td>\n",
       "      <td>10.484740</td>\n",
       "      <td>2.376818</td>\n",
       "      <td>5.944450</td>\n",
       "      <td>5.488851</td>\n",
       "      <td>2.672394</td>\n",
       "      <td>3.550734</td>\n",
       "      <td>...</td>\n",
       "      <td>9.493468</td>\n",
       "      <td>5.833063</td>\n",
       "      <td>3.530834</td>\n",
       "      <td>8.579912</td>\n",
       "      <td>2.825698</td>\n",
       "      <td>4.247598</td>\n",
       "      <td>2.937310</td>\n",
       "      <td>2.300193</td>\n",
       "      <td>2.298061</td>\n",
       "      <td>7.700181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>79</td>\n",
       "      <td>2.376699</td>\n",
       "      <td>6.037057</td>\n",
       "      <td>12.841133</td>\n",
       "      <td>11.024409</td>\n",
       "      <td>2.376818</td>\n",
       "      <td>5.704645</td>\n",
       "      <td>5.833775</td>\n",
       "      <td>2.672595</td>\n",
       "      <td>3.030480</td>\n",
       "      <td>...</td>\n",
       "      <td>9.388493</td>\n",
       "      <td>6.600218</td>\n",
       "      <td>3.477926</td>\n",
       "      <td>8.592740</td>\n",
       "      <td>2.825698</td>\n",
       "      <td>2.339240</td>\n",
       "      <td>2.733030</td>\n",
       "      <td>2.300193</td>\n",
       "      <td>2.298061</td>\n",
       "      <td>7.824368</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80 rows Ã— 15980 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0     RAB30      NFIC    HNRNPA1       HECA    ADRA2A     DHX57  \\\n",
       "0            0  3.176484  6.217651  12.932333  11.649055  3.307900  6.111577   \n",
       "1            1  3.337685  7.084601  12.957544  11.407211  2.277668  6.094954   \n",
       "2            2  2.709277  6.672811  12.928799  11.676965  2.372775  6.032964   \n",
       "3            3  3.176484  6.148654  13.083771  11.841469  2.536921  6.111577   \n",
       "4            4  3.038200  7.006673  12.880925  11.335277  2.295058  6.051544   \n",
       "..         ...       ...       ...        ...        ...       ...       ...   \n",
       "75          75  2.376699  5.598443  12.734216  10.948214  2.357735  5.895650   \n",
       "76          76  2.376699  5.971997  12.778174  10.910101  2.544198  6.092561   \n",
       "77          77  2.376699  6.298010  12.562975  10.827222  2.376818  5.776021   \n",
       "78          78  2.376699  6.177901  12.437447  10.484740  2.376818  5.944450   \n",
       "79          79  2.376699  6.037057  12.841133  11.024409  2.376818  5.704645   \n",
       "\n",
       "       YIPF4     LAGE3   SLC11A2  ...      ATP5I      TCHP     HDAC4  \\\n",
       "0   6.846514  3.172566  3.547691  ...  10.696304  7.192669  5.052599   \n",
       "1   6.119469  4.326554  4.386112  ...  10.729904  7.130932  4.709394   \n",
       "2   7.695579  3.658660  4.261204  ...  10.289376  6.888741  4.257237   \n",
       "3   8.008814  3.105723  3.306101  ...  10.899441  8.645110  4.538814   \n",
       "4   6.335378  4.129643  4.083541  ...  10.601442  7.433140  4.051288   \n",
       "..       ...       ...       ...  ...        ...       ...       ...   \n",
       "75  5.775188  2.671846  3.357576  ...   9.409788  6.993906  3.580868   \n",
       "76  6.061663  2.700400  3.211287  ...   9.098858  6.457562  3.196260   \n",
       "77  5.775158  2.659445  3.040549  ...   9.332262  6.662107  3.333846   \n",
       "78  5.488851  2.672394  3.550734  ...   9.493468  5.833063  3.530834   \n",
       "79  5.833775  2.672595  3.030480  ...   9.388493  6.600218  3.477926   \n",
       "\n",
       "        AP2M1   CYP11B1     CDYL2      INMT     WDR93      DNM3     RBBP4  \n",
       "0   10.142164  3.150822  3.205546  2.909560  2.281185  2.224344  6.011788  \n",
       "1   10.422971  2.664382  2.238226  2.653035  2.399193  2.234680  7.785639  \n",
       "2    9.830656  2.664382  2.238226  2.639401  2.235161  2.954536  8.514379  \n",
       "3    9.461622  2.661650  2.275715  2.887727  2.281185  2.224344  6.682733  \n",
       "4    9.726606  2.827227  2.238226  2.809803  2.235161  2.234680  8.031372  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "75   7.977729  2.462509  2.298061  2.937310  2.300193  2.298061  8.283852  \n",
       "76   8.442538  2.811841  2.329875  2.937310  2.300193  2.298061  8.444267  \n",
       "77   8.287376  3.020280  2.329875  2.306937  2.300193  2.298061  7.939047  \n",
       "78   8.579912  2.825698  4.247598  2.937310  2.300193  2.298061  7.700181  \n",
       "79   8.592740  2.825698  2.339240  2.733030  2.300193  2.298061  7.824368  \n",
       "\n",
       "[80 rows x 15980 columns]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare and scale the data. \n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=25)\n",
    "\n",
    "x_train = StandardScaler().fit_transform(x_train)\n",
    "x_test = StandardScaler().fit_transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------------------------------------\n",
    "def make_that_model(layers, loss_function, optimizer, learning_rate, exit_activation=None):\n",
    "    model = keras.Sequential()\n",
    "    for layer in layers:\n",
    "        model.add(getattr(keras.layers, layer[\"name\"])(**layer[\"options\"]))\n",
    "\n",
    "    if exit_activation!=None:\n",
    "        model.add(Activation(exit_activation))\n",
    "\n",
    "    op = getattr(keras.optimizers, optimizer)(learning_rate=(learning_rate))\n",
    "    model.compile(loss=loss_function, optimizer=op, metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "#--------------------------------------------------------------------------\n",
    "def model_create(parameterization, load_file=None):\n",
    "\n",
    "  model = make_that_model(\n",
    "      parameterization.get('layers'),\n",
    "      parameterization.get('loss_function'),\n",
    "      parameterization.get('optimizer'),\n",
    "      parameterization.get('learning_rate'),\n",
    "      )\n",
    "  \n",
    "  if load_file != None:\n",
    "    model.load_weights(load_file)\n",
    "  \n",
    "  return model\n",
    "#--------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"layers\": [\n",
    "      {\n",
    "        \"name\": \"BatchNormalization\",\n",
    "        \"options\": {\n",
    "          \"input_shape\": [\n",
    "            15979\n",
    "          ]\n",
    "        }\n",
    "      },\n",
    "      {\n",
    "        \"name\": \"BatchNormalization\",\n",
    "        \"options\": {}\n",
    "      },\n",
    "      {\n",
    "        \"name\": \"BatchNormalization\",\n",
    "        \"options\": {}\n",
    "      },\n",
    "      {\n",
    "        \"name\": \"BatchNormalization\",\n",
    "        \"options\": {}\n",
    "      },\n",
    "      {\n",
    "        \"name\": \"Dense\",\n",
    "        \"options\": {\n",
    "          \"units\": 121,\n",
    "          \"activation\": \"elu\",\n",
    "          \"use_bias\": True\n",
    "        }\n",
    "      },\n",
    "      {\n",
    "        \"name\": \"Dense\",\n",
    "        \"options\": {\n",
    "          \"units\": 4,\n",
    "          \"activation\": \"softmax\"\n",
    "        }\n",
    "      }\n",
    "    ],\n",
    "    \"loss_function\": \"binary_crossentropy\",\n",
    "    \"optimizer\": \"Nadam\",\n",
    "    \"batch_size\": 64,\n",
    "    \"learning_rate\": 0.04867806312636558,\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_3 (Batch (None, 15979)             63916     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 15979)             63916     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 15979)             63916     \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 15979)             63916     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 121)               1933580   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 488       \n",
      "=================================================================\n",
      "Total params: 2,189,732\n",
      "Trainable params: 2,061,900\n",
      "Non-trainable params: 127,832\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = model_create(config)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 2/200\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 3/200\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 4/200\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 5/200\n",
      "2/2 [==============================] - 1s 394ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 6/200\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 7/200\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 8/200\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 9/200\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 10/200\n",
      "2/2 [==============================] - 0s 194ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 11/200\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 12/200\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 13/200\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 14/200\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 15/200\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 16/200\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 17/200\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 18/200\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 19/200\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 20/200\n",
      "2/2 [==============================] - 0s 128ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 21/200\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 22/200\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 23/200\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 24/200\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 25/200\n",
      "2/2 [==============================] - 0s 113ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 26/200\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 27/200\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 28/200\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 29/200\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 30/200\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 31/200\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 32/200\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 33/200\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 34/200\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 35/200\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 36/200\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 37/200\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 38/200\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 39/200\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 40/200\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 41/200\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 42/200\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 43/200\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 44/200\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 45/200\n",
      "2/2 [==============================] - 0s 108ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 46/200\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 47/200\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 48/200\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 49/200\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 50/200\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 51/200\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 52/200\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 53/200\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.00 - 0s 37ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 54/200\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 55/200\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 56/200\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 57/200\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 58/200\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 59/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 60/200\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 61/200\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 62/200\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 63/200\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 64/200\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 65/200\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 66/200\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 67/200\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 68/200\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 69/200\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 70/200\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 71/200\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 72/200\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 73/200\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 74/200\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 75/200\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 76/200\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 77/200\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 78/200\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 79/200\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 80/200\n",
      "2/2 [==============================] - 0s 110ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 81/200\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 82/200\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 83/200\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 84/200\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 85/200\n",
      "2/2 [==============================] - 0s 105ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 86/200\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 87/200\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 88/200\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 89/200\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 90/200\n",
      "2/2 [==============================] - 0s 128ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 91/200\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 92/200\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 93/200\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 94/200\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 95/200\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 96/200\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 97/200\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 98/200\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 99/200\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 100/200\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 101/200\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 102/200\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 103/200\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 104/200\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 105/200\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 106/200\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 107/200\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 108/200\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 109/200\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 110/200\n",
      "2/2 [==============================] - 0s 105ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 111/200\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 112/200\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 113/200\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 114/200\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 115/200\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 116/200\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 117/200\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 118/200\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 119/200\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 120/200\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 121/200\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 122/200\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 123/200\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 124/200\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 125/200\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 126/200\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 127/200\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 128/200\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 129/200\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 130/200\n",
      "2/2 [==============================] - 0s 109ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 131/200\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 132/200\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 133/200\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 134/200\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 135/200\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 136/200\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 137/200\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 138/200\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 139/200\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 140/200\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 141/200\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 142/200\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 143/200\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 144/200\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 145/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 144ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 146/200\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 147/200\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 148/200\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 149/200\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 150/200\n",
      "2/2 [==============================] - 0s 120ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 151/200\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 152/200\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 153/200\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 154/200\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 155/200\n",
      "2/2 [==============================] - 0s 105ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 156/200\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 157/200\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 158/200\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 159/200\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 160/200\n",
      "2/2 [==============================] - 0s 179ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 161/200\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 162/200\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 163/200\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 164/200\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 165/200\n",
      "2/2 [==============================] - 1s 614ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 166/200\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 167/200\n",
      "2/2 [==============================] - 1s 367ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 168/200\n",
      "2/2 [==============================] - 1s 116ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 169/200\n",
      "2/2 [==============================] - 1s 246ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 170/200\n",
      "2/2 [==============================] - 2s 1s/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 171/200\n",
      "2/2 [==============================] - 1s 342ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 172/200\n",
      "2/2 [==============================] - 1s 146ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 173/200\n",
      "2/2 [==============================] - 1s 267ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 174/200\n",
      "2/2 [==============================] - 1s 80ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 175/200\n",
      "2/2 [==============================] - 2s 1s/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 176/200\n",
      "2/2 [==============================] - 1s 311ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 177/200\n",
      "2/2 [==============================] - 1s 341ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 178/200\n",
      "2/2 [==============================] - 1s 249ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 179/200\n",
      "2/2 [==============================] - 1s 343ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 180/200\n",
      "2/2 [==============================] - 3s 2s/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 181/200\n",
      "2/2 [==============================] - 1s 490ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 182/200\n",
      "2/2 [==============================] - 2s 625ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 183/200\n",
      "2/2 [==============================] - 1s 374ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 184/200\n",
      "2/2 [==============================] - 1s 443ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 185/200\n",
      "2/2 [==============================] - 3s 2s/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 186/200\n",
      "2/2 [==============================] - 1s 335ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 187/200\n",
      "2/2 [==============================] - 1s 437ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 188/200\n",
      "2/2 [==============================] - 1s 225ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 189/200\n",
      "2/2 [==============================] - 1s 361ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 190/200\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 191/200\n",
      "2/2 [==============================] - 1s 439ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 192/200\n",
      "2/2 [==============================] - 1s 261ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 193/200\n",
      "2/2 [==============================] - 1s 443ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 194/200\n",
      "2/2 [==============================] - 1s 597ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 195/200\n",
      "2/2 [==============================] - 2s 2s/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 196/200\n",
      "2/2 [==============================] - 1s 236ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 197/200\n",
      "2/2 [==============================] - 1s 440ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 198/200\n",
      "2/2 [==============================] - 1s 131ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 199/200\n",
      "2/2 [==============================] - 1s 511ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 200/200\n",
      "2/2 [==============================] - 3s 2s/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(\n",
    "        x=x_train, y=y_train, \n",
    "        validation_data=(x_test,y_test),\n",
    "        validation_freq=5,\n",
    "#         workers=8,\n",
    "#         use_multiprocessing=True,\n",
    "        epochs=200, \n",
    "        verbose=1,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=30\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 680ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "stat = classification_report(np.argmax(pred,-1), np.argmax(y_test, -1), output_dict=True)\n",
    "metrics = dict(\n",
    "        zip(model.metrics_names,\n",
    "            model.evaluate(x=x_test, y=y_test, batch_size=batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': 0.0, 'accuracy': 1.0}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 4},\n",
       " '1': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 5},\n",
       " '3': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 3},\n",
       " 'accuracy': 1.0,\n",
       " 'macro avg': {'precision': 1.0,\n",
       "  'recall': 1.0,\n",
       "  'f1-score': 1.0,\n",
       "  'support': 12},\n",
       " 'weighted avg': {'precision': 1.0,\n",
       "  'recall': 1.0,\n",
       "  'f1-score': 1.0,\n",
       "  'support': 12}}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['disease: Rheumatoid Arthiritis (DMARD-IR)',\n",
       "       'disease: Rheumatoid Arthiritis (DMARD-IR)',\n",
       "       'disease: Rheumatoid Arthiritis (DMARD-IR)',\n",
       "       'disease: SLE (Systemic LUPUS Erythomatosus)',\n",
       "       'disease: Rheumatoid Arthiritis (DMARD-IR)', 'disease: Control',\n",
       "       'disease: SLE (Systemic LUPUS Erythomatosus)',\n",
       "       'disease: SLE (Systemic LUPUS Erythomatosus)', 'disease: Control',\n",
       "       'disease: Control', 'disease: Control',\n",
       "       'disease: Rheumatoid Arthiritis (DMARD-IR)'], dtype=object)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label.inverse_transform(np.argmax(pred, axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['disease: Rheumatoid Arthiritis (DMARD-IR)',\n",
       "       'disease: Rheumatoid Arthiritis (DMARD-IR)',\n",
       "       'disease: Rheumatoid Arthiritis (DMARD-IR)',\n",
       "       'disease: SLE (Systemic LUPUS Erythomatosus)',\n",
       "       'disease: Rheumatoid Arthiritis (DMARD-IR)', 'disease: Control',\n",
       "       'disease: SLE (Systemic LUPUS Erythomatosus)',\n",
       "       'disease: SLE (Systemic LUPUS Erythomatosus)', 'disease: Control',\n",
       "       'disease: Control', 'disease: Control',\n",
       "       'disease: Rheumatoid Arthiritis (DMARD-IR)'], dtype=object)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label.inverse_transform(np.argmax(y_test, axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
